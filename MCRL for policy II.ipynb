{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# monte carlo code for policyII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "from collections import defaultdict\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    st= [0]*16\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(action,st,i): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    \n",
    "    if action == 0 :\n",
    "        if f <= st[i]: # fail\n",
    "            st[i+8]=1\n",
    "            reward= -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "        else:\n",
    "            #st[i+8]=0\n",
    "            st[i] +=5\n",
    "            reward = 5\n",
    "            \n",
    "    if action ==1 :\n",
    "        if st[i+8] ==0 : \n",
    "            reward = -(time_interval / tp[i])*tp[i]\n",
    "        else :\n",
    "            reward = -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "            \n",
    "        st[i]=0\n",
    "        st[i+8]=0\n",
    "            \n",
    "           \n",
    "    return (tuple(st) , reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1, 0, 1, 1, 1, 11, 1, 1, 11, 0, 1, 0, 1, 1, 1), -40.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env(1,list((1,1,1,1,1,1,11,1,1,11,1,1,0,1,1,1)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pi function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = [0,1]\n",
    "def policy_using_pi(St, pi):\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi[(St,a)] for a in ALL_POSSIBLE_ACTIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state , pi,i):\n",
    "    if(state[i+8]==1):\n",
    "        return 1\n",
    "    else:\n",
    "        st = (state[i],state[i+8])\n",
    "        return policy_using_pi(st, pi)  #epsilon_soft "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.50\n",
    "def play_episode(pi,i):\n",
    "    #S0,A0,R1,S1,A1,R2,S2,A2,R3 ,...\n",
    "    s= reset()\n",
    "    a= choose_action(s, pi ,i)\n",
    "    \n",
    "    #r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [((s, a, 0))]\n",
    "    \n",
    "    for j in range(running_time//time_interval +1):\n",
    "    \n",
    "        s , r = env(a,list(s),i)\n",
    "        if (j == running_time//time_interval):\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "        else:\n",
    "            a= choose_action(s, pi ,i)\n",
    "            states_actions_rewards.append((s,a, r))\n",
    "            \n",
    "    \n",
    "     # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        \n",
    "       # the value of the terminal state is 0 by definition\n",
    "       # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    \n",
    "    return states_actions_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 0  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2350])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 1  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([985])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print(current_state[i] ,action,r,obs[i])\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for wheel Rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 2  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([695])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 3  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1375])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 4  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([345])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=4\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 5  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3860])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for steering wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 6  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([780])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 7  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+8])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2020])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=7\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+8])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),i)\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
