{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCRL for policyIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "from collections import defaultdict\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    st= [0]*17\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(random.choices([0,1], weights=(99.70,0.30), k=20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env (action,st,step,i): \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    reward =[]\n",
    "    \n",
    "    if step == 5000:#scheduled overhaul \n",
    "        for j in range(17):\n",
    "            st[j] = 0\n",
    "        st[8]=1 \n",
    "        reward= -(time_interval / tp[i])*time_interval * math.ceil(0.8 *sum(tp)/time_interval) \n",
    "    else :    \n",
    "        if action == 0 :\n",
    "            if f <= st[i]:\n",
    "                st[i+9]=1 #fail\n",
    "                reward = -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "                \n",
    "            else:\n",
    "                #st[i+9]=0\n",
    "                st[i] +=5 #age\n",
    "                reward = 5\n",
    "        \n",
    "        if action ==1 :\n",
    "            if st[i+9]==0 :\n",
    "                reward= -(time_interval / tp[i])*tp[i]\n",
    "            else:\n",
    "                reward= -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "            st[i]=0\n",
    "            st[i+9]=0\n",
    "            \n",
    "            \n",
    "    \n",
    "    return tuple(st) , reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pi function \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = [0,1]\n",
    "def policy_using_pi(St, pi):\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi[(St,a)] for a in ALL_POSSIBLE_ACTIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state , pi,i):\n",
    "    if (state[i+9]==1):\n",
    "        return  1  #repleace with probability 1\n",
    "    else: \n",
    "        st = (state[i],state[i+9])\n",
    "        return policy_using_pi(st, pi)  #epsilon_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "choose_action((25,1,20,14,10,20,50,40,0,1,0,0,0,0,0,0,0), pi,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.50\n",
    "\n",
    "def play_episode(pi,i):\n",
    "    #S0,A0,R1,S1,A1,R2,S2,A2,R3 ,...\n",
    "    s= reset()\n",
    "    a= choose_action(s, pi ,i)\n",
    "    step = 0\n",
    "    #r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [((s, a, 0))]\n",
    "    \n",
    "    for j in range(running_time//time_interval +1):\n",
    "        \n",
    "        s , r = env(a,list(s),step,i)\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "        if (j == running_time//time_interval):\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "        else:\n",
    "            a= choose_action(s, pi ,i)\n",
    "            states_actions_rewards.append((s,a, r))\n",
    "            \n",
    "    \n",
    "     # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        \n",
    "       # the value of the terminal state is 0 by definition\n",
    "       # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    \n",
    "    return states_actions_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 0  \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2350])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 1 \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([990])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wheel rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 2 \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([695])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 3 \n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1370])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 4\n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([345])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=4\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 5\n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3835])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=5\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 6\n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([775])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=6\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "Q = np.zeros((100000 ,2) + (2,))\n",
    "returns =np.zeros((100000 ,2) + (2,))\n",
    "N = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 7\n",
    "\n",
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi,i)\n",
    "    # calculate Q(s,a)\n",
    "    seen_state_action_pairs = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t = (s[i] ,s[i+9])\n",
    "        state_action = (s_t,a)\n",
    "        \n",
    "        if state_action not in seen_state_action_pairs:\n",
    "            \n",
    "            returns[s_t][a] += G\n",
    "            N[s_t][a] +=1\n",
    "                \n",
    "            Q[s_t][a] = returns[s_t][a] /N[s_t][a] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q[s_t])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2025])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=7\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    \n",
    "    action = np.argmax(Q[(current_state[i],current_state[i+9])])\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state[i])\n",
    "    obs , r = env(action,list(current_state),step,i)\n",
    "    step +=5\n",
    "    if step == 5005:\n",
    "        step = 0\n",
    "    #print (current_state[i] ,action,r,obs[i] )\n",
    "    current_state = obs\n",
    "np.unique(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
