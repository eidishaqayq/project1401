{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCRL for policy IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    st= [0]*16\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = [[0,2],[2,7],[2,4],[6,5],[3,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def member(i , fi):\n",
    "    mem=[]\n",
    "    if i in fi[0]:\n",
    "        if i !=fi[0][0]:\n",
    "            mem.append(fi[0][0])\n",
    "        if i !=fi[0][1]:\n",
    "            mem.append(fi[0][1])\n",
    "        \n",
    "    if i in fi[1]:\n",
    "        if i !=fi[1][0]:\n",
    "            mem.append(fi[1][0])\n",
    "        if i !=fi[1][1]:\n",
    "            mem.append(fi[1][1])\n",
    "        \n",
    "    if i in fi[2]:\n",
    "        if i !=fi[2][0]:\n",
    "            mem.append(fi[2][0])\n",
    "        if i !=fi[2][1]:\n",
    "            mem.append(fi[2][1])\n",
    "    \n",
    "    if i in fi[3]:\n",
    "        if i !=fi[3][0]:\n",
    "            mem.append(fi[3][0])\n",
    "        if i !=fi[3][1]:\n",
    "            mem.append(fi[3][1])\n",
    "    \n",
    "    if i in fi[4]:\n",
    "        if i !=fi[4][0]:\n",
    "            mem.append(fi[4][0])\n",
    "        if i !=fi[4][1]:\n",
    "            mem.append(fi[4][1])\n",
    "    \n",
    "    return mem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member(1,fi)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list):\n",
    " \n",
    "    # Creating an empty dictionary\n",
    "    freq = {}\n",
    "    for item in my_list:\n",
    "        if (item in freq):\n",
    "            freq[item] += 1\n",
    "        else:\n",
    "            freq[item] = 1\n",
    " \n",
    "    for key, value in freq.items():\n",
    "        print (\"% d : % d\"%(key, value))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(action,st):\n",
    "    reward = np.zeros(8)\n",
    "    failure_time = np.zeros(8)\n",
    "    \n",
    "    for a in range(8):\n",
    "        if (action[a] == 1):\n",
    "            for j in range(len(member(a,fi))):\n",
    "                action[member(a,fi)[j]] = 1\n",
    "                \n",
    "            \n",
    "        failure_time[a] = random.weibullvariate(weibull_scale[a],weibull_shape[a])\n",
    "\n",
    "        if failure_time[a] <= st[a]: # fail\n",
    "            st[a+8]=1\n",
    "\n",
    "            tpi = 0 \n",
    "            for j in range(len(member(a,fi))):\n",
    "                tpi+=tp[member(a,fi)[j]]\n",
    "\n",
    "            tfi = 0\n",
    "            for j in range(len(member(a,fi))):\n",
    "                tfi+=tf[member(a,fi)[j]]\n",
    "\n",
    "            reward[a] =- (time_interval/(0.8*time_interval*sum((tpi,tp[a]))))*time_interval*math.ceil(0.8*sum((tfi,tf[a]))/time_interval)\n",
    "\n",
    "            \n",
    "        else :\n",
    "            #st[a+8]=0\n",
    "            st[a] +=5\n",
    "            reward[a] = 5\n",
    "            \n",
    "            \n",
    "        if action[a]==1:            \n",
    "            if (st[a+8]==0):\n",
    "                tpi = 0 \n",
    "                for j in range(len(member(a,fi))):\n",
    "                    tpi+=tp[member(a,fi)[j]]\n",
    "                reward[a] = - (time_interval/(0.8*time_interval*sum((tpi,tp[a]))))*0.8*sum((tpi,tp[a]))\n",
    "            \n",
    "            if (st[a+8]==1):\n",
    "                tpi = 0 \n",
    "                for j in range(len(member(a,fi))):\n",
    "                    tpi+=tp[member(a,fi)[j]]\n",
    "\n",
    "                tfi = 0\n",
    "                for j in range(len(member(a,fi))):\n",
    "                    tfi+=tf[member(a,fi)[j]]\n",
    "\n",
    "                reward[a] =- (time_interval/(0.8*time_interval*sum((tpi,tp[a]))))*time_interval*math.ceil(0.8*sum((tfi,tf[a]))/time_interval)\n",
    "\n",
    "                \n",
    "            st[a]=0\n",
    "            st[a+8]=0  \n",
    "\n",
    "    \n",
    "    return (tuple(st) , reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((805, 0, 25, 0, 15, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " array([ 5., -1.,  5., -1.,  5., -1., -1.,  5.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env(list((0,1,0,0,0,1,0,0)),list((800,1,20,14,10,20,50,1,0,0,0,0,0,0,0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pi function for each components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state , pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears):\n",
    "    action = np.zeros(8)\n",
    "    \n",
    "    for i in range(8):\n",
    "        \n",
    "        if state[i+8]==1:\n",
    "            action[i] = 1\n",
    "            \n",
    "        else:\n",
    "            st = (state[i],state[i+8])\n",
    "            if i == 0:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_tire[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i== 1:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_transmissin[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 2:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_wheel[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 3:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_coupling[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 4:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_motor[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 5:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_brake[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 6:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_steering[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "            if i == 7:\n",
    "                action[i]= np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi_gears[(st,a)] for a in ALL_POSSIBLE_ACTIONS])\n",
    "    return action    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_tire = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_transmissin= defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_wheel = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_coupling = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_motor = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_brake = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_steering = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))\n",
    "pi_gears = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_tire = np.zeros((100000 ,2) + (2,))\n",
    "Q_transmissin = np.zeros((100000 ,2) + (2,))\n",
    "Q_wheel = np.zeros((100000 ,2) + (2,))\n",
    "Q_coupling = np.zeros((100000 ,2) + (2,))\n",
    "Q_motor = np.zeros((100000 ,2) + (2,))\n",
    "Q_brake = np.zeros((100000 ,2) + (2,))\n",
    "Q_steering = np.zeros((100000 ,2) + (2,))\n",
    "Q_gears = np.zeros((100000 ,2) + (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_tire =np.zeros((100000 ,2) + (2,))\n",
    "returns_transmissin =np.zeros((100000 ,2) + (2,))\n",
    "returns_wheel =np.zeros((100000 ,2) + (2,))\n",
    "returns_coupling =np.zeros((100000 ,2) + (2,))\n",
    "returns_motor =np.zeros((100000 ,2) + (2,))\n",
    "returns_brake =np.zeros((100000 ,2) + (2,))\n",
    "returns_steering =np.zeros((100000 ,2) + (2,))\n",
    "returns_gears =np.zeros((100000 ,2) + (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tire = np.zeros((100000 ,2) + (2,))\n",
    "N_transmissin = np.zeros((100000 ,2) + (2,))\n",
    "N_wheel = np.zeros((100000 ,2) + (2,))\n",
    "N_coupling = np.zeros((100000 ,2) + (2,))\n",
    "N_motor = np.zeros((100000 ,2) + (2,))\n",
    "N_brake = np.zeros((100000 ,2) + (2,))\n",
    "N_steering = np.zeros((100000 ,2) + (2,))\n",
    "N_gears = np.zeros((100000 ,2) + (2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=choose_action(list((800,1,20,14,10,20,50,1,1,0,0,0,0,0,0,0)), pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
       " array([-6.09756098,  5.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        ]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env(a,list((800,1,20,14,10,20,50,1,1,0,0,0,0,0,0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.50\n",
    "def play_episode(pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears):\n",
    "    #S0,A0,R1,S1,A1,R2,S2,A2,R3 ,...\n",
    "    s= reset()\n",
    "    a= choose_action(s,pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "    \n",
    "    #r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [((s, a, 0))]\n",
    "    \n",
    "    for j in range(running_time//time_interval +1):\n",
    "    \n",
    "        s , r = env(a,list(s))\n",
    "        if (j == running_time//time_interval):\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "        else:\n",
    "            a= choose_action(s,pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "            states_actions_rewards.append((s,a, r))\n",
    "            \n",
    "    \n",
    "     # calculate the returns by working backwards from the terminal state\n",
    "    G = np.zeros(8)\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        \n",
    "       # the value of the terminal state is 0 by definition\n",
    "       # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    \n",
    "    return states_actions_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play_episode(pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transmission and coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "    # calculate Q(s,a)\n",
    "    \n",
    "    seen_state_action_pairs_transmissin = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_transmissin = (s[1] ,s[1+8])\n",
    "        state_action_transmissin = (s_t_transmissin,int(a[1]))\n",
    "        \n",
    "        if state_action_transmissin not in seen_state_action_pairs_transmissin:\n",
    "            \n",
    "            returns_transmissin[s_t_transmissin][int(a[1])] += G[1]\n",
    "            N_transmissin[s_t_transmissin][int(a[1])] +=1\n",
    "                \n",
    "            Q_transmissin[s_t_transmissin][int(a[1])] = returns_transmissin[s_t_transmissin][int(a[1])] /N_transmissin[s_t_transmissin][int(a[1])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_transmissin.add(state_action_transmissin)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_transmissin[s_t_transmissin])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_transmissin[(s_t_transmissin,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_transmissin[(s_t_transmissin,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "    \n",
    "    \n",
    "    seen_state_action_pairs_coupling = set()\n",
    "    \n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_coupling = (s[3] ,s[3+8])\n",
    "        state_action_coupling = (s_t_coupling,int(a[3]))\n",
    "        \n",
    "        if state_action_coupling not in seen_state_action_pairs_coupling:\n",
    "            \n",
    "            returns_coupling[s_t_coupling][int(a[3])] += G[3]\n",
    "            N_coupling[s_t_coupling][int(a[3])] +=1\n",
    "                \n",
    "            Q_coupling[s_t_coupling][int(a[3])] = returns_coupling[s_t_coupling][int(a[3])] /N_coupling[s_t_coupling][int(a[3])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_coupling.add(state_action_coupling)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_coupling[s_t_coupling])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_coupling[(s_t_coupling,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_coupling[(s_t_coupling,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac (current_state):\n",
    "    action = np.zeros(8)\n",
    "    action[1] = np.argmax(Q_transmissin[(current_state[1],current_state[1+8])])\n",
    "    action[3] = np.argmax(Q_coupling[(current_state[3],current_state[3+8])])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 990 :  128\n",
      " 975 :  19\n",
      " 980 :  28\n",
      " 945 :  1\n",
      " 985 :  45\n",
      " 970 :  16\n",
      " 965 :  7\n",
      " 960 :  4\n",
      " 950 :  2\n",
      " 955 :  1\n",
      " 920 :  1\n"
     ]
    }
   ],
   "source": [
    "#transmission\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[1,3]] = max(ac(current_state)[[1,3]])\n",
    "    if action[1] ==1:\n",
    "        time_replace.append(current_state[1])\n",
    "    obs , r = env(action,list(current_state))\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 990 :  173\n",
      " 980 :  22\n",
      " 985 :  34\n",
      " 975 :  10\n",
      " 965 :  3\n",
      " 945 :  2\n",
      " 970 :  5\n",
      " 930 :  1\n",
      " 935 :  1\n",
      " 950 :  1\n"
     ]
    }
   ],
   "source": [
    "#coupling\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[1,3]] = max(ac(current_state)[[1,3]])\n",
    "    if action[3] ==1:\n",
    "        time_replace.append(current_state[3])\n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[5] ,action[5],r[5],obs[5] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brake and sterring wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(1000 +1):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "    # calculate Q(s,a)\n",
    "    \n",
    "    seen_state_action_pairs_brake = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_brake = (s[5] ,s[5+8])\n",
    "        state_action_brake = (s_t_brake,int(a[5]))\n",
    "        \n",
    "        if state_action_brake not in seen_state_action_pairs_brake:\n",
    "            \n",
    "            returns_brake[s_t_brake][int(a[5])] += G[5]\n",
    "            N_brake[s_t_brake][int(a[5])] +=1\n",
    "                \n",
    "            Q_brake[s_t_brake][int(a[5])] = returns_brake[s_t_brake][int(a[5])] /N_brake[s_t_brake][int(a[5])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_brake.add(state_action_brake)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_brake[s_t_brake])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_brake[(s_t_brake,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_brake[(s_t_brake,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "    \n",
    "    \n",
    "    seen_state_action_pairs_steering = set()\n",
    "    \n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_steering = (s[6] ,s[6+8])\n",
    "        state_action_steering = (s_t_steering,int(a[6]))\n",
    "        \n",
    "        if state_action_steering not in seen_state_action_pairs_steering:\n",
    "            \n",
    "            returns_steering[s_t_steering][int(a[6])] += G[6]\n",
    "            N_steering[s_t_steering][int(a[6])] +=1\n",
    "                \n",
    "            Q_steering[s_t_steering][int(a[6])] = returns_steering[s_t_steering][int(a[6])] /N_steering[s_t_steering][int(a[6])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_steering.add(state_action_steering)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_steering[s_t_steering])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_steering[(s_t_steering,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_steering[(s_t_steering,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac (current_state):\n",
    "    action = np.zeros(8)\n",
    "    action[5] = np.argmax(Q_brake[(current_state[5],current_state[5+8])])\n",
    "    action[6] = np.argmax(Q_steering[(current_state[6],current_state[6+8])])        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 790 :  234\n",
      " 770 :  8\n",
      " 760 :  5\n",
      " 750 :  3\n",
      " 775 :  11\n",
      " 765 :  5\n",
      " 785 :  14\n",
      " 780 :  19\n",
      " 755 :  4\n",
      " 745 :  3\n",
      " 740 :  4\n",
      " 730 :  1\n",
      " 735 :  2\n",
      " 725 :  2\n",
      " 715 :  1\n"
     ]
    }
   ],
   "source": [
    "#brake\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[5,6]] = max(ac(current_state)[[5,6]])\n",
    "    if action[5] ==1:\n",
    "        time_replace.append(current_state[5])\n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[5],current_state[6] ,action[5],r[5],obs[5] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)\n",
    "#Q_brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 865 :  29\n",
      " 790 :  140\n",
      " 860 :  44\n",
      " 855 :  8\n",
      " 850 :  1\n"
     ]
    }
   ],
   "source": [
    "#steering\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "#for j in range(20000):\n",
    "for j in range(50000):\n",
    "    action[[5,6]] = max(ac(current_state)[[5,6]])\n",
    "    if action[6] ==1:\n",
    "        time_replace.append(current_state[6])\n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[6] ,action[6],r[6],obs[6] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tire and Wheel rim and motor and shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(1000):# Looping through episodes \n",
    "    \n",
    "    epsilon = 1/(epi+1)\n",
    "    # generate an episode using pi\n",
    "    states_actions_returns = play_episode(pi_tire ,pi_transmissin, pi_wheel, pi_coupling, pi_motor, pi_brake, pi_steering, pi_gears)\n",
    "    # calculate Q(s,a)\n",
    "    \n",
    "    seen_state_action_pairs_tire = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_tire = (s[0] ,s[0+8])\n",
    "        state_action_tire = (s_t_tire,int(a[0]))\n",
    "        \n",
    "        if state_action_tire not in seen_state_action_pairs_tire:\n",
    "            \n",
    "            returns_tire[s_t_tire][int(a[0])] += G[0]\n",
    "            N_tire[s_t_tire][int(a[0])] +=1\n",
    "                \n",
    "            Q_tire[s_t_tire][int(a[0])] = returns_brake[s_t_tire][int(a[0])] /N_tire[s_t_tire][int(a[0])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_tire.add(state_action_tire)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_tire[s_t_tire])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_tire[(s_t_tire,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_tire[(s_t_tire,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "    \n",
    "    \n",
    "    seen_state_action_pairs_wheel = set()\n",
    "    \n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_wheel = (s[2] ,s[2+8])\n",
    "        state_action_wheel = (s_t_wheel,int(a[2]))\n",
    "        \n",
    "        if state_action_wheel not in seen_state_action_pairs_wheel:\n",
    "            \n",
    "            returns_wheel[s_t_wheel][int(a[2])] += G[2]\n",
    "            N_wheel[s_t_wheel][int(a[2])] +=1\n",
    "                \n",
    "            Q_wheel[s_t_wheel][int(a[2])] = returns_wheel[s_t_wheel][int(a[2])] /N_wheel[s_t_wheel][int(a[2])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_wheel.add(state_action_wheel)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_wheel[s_t_wheel])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_wheel[(s_t_wheel,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_wheel[(s_t_wheel,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "     \n",
    "    seen_state_action_pairs_motor = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_motor = (s[4] ,s[4+8])\n",
    "        state_action_motor = (s_t_motor,int(a[4]))\n",
    "        \n",
    "        if state_action_motor not in seen_state_action_pairs_motor:\n",
    "            \n",
    "            returns_motor[s_t_motor][int(a[4])] += G[4]\n",
    "            N_motor[s_t_motor][int(a[4])] +=1\n",
    "                \n",
    "            Q_motor[s_t_motor][int(a[4])] = returns_motor[s_t_motor][int(a[4])] /N_motor[s_t_motor][int(a[4])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_motor.add(state_action_motor)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_motor[s_t_motor])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_motor[(s_t_motor,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_motor[(s_t_motor,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "     \n",
    "    seen_state_action_pairs_gears = set()\n",
    "    for s, a, G in states_actions_returns:\n",
    "        # check if we have already seen s\n",
    "        # called \"first-visit\" MC policy evaluation\n",
    "        s_t_gears = (s[7] ,s[7+8])\n",
    "        state_action_gears = (s_t_gears,int(a[7]))\n",
    "        \n",
    "        if state_action_gears not in seen_state_action_pairs_gears:\n",
    "            \n",
    "            returns_gears[s_t_gears][int(a[7])] += G[7]\n",
    "            N_gears[s_t_gears][int(a[7])] +=1\n",
    "                \n",
    "            Q_gears[s_t_gears][int(a[7])] = returns_gears[s_t_gears][int(a[7])] /N_gears[s_t_gears][int(a[7])] # Average reward across episodes\n",
    "            \n",
    "            seen_state_action_pairs_gears.add(state_action_gears)\n",
    "                \n",
    "            #for each s in the episode         \n",
    "            A_star = np.argmax(Q_gears[s_t_gears])# Finding the action with maximum value        \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi_gears[(s_t_gears,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi_gears[(s_t_gears,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ac (current_state):\n",
    "    action = np.zeros(8)\n",
    "    if current_state[7+8]==1:\n",
    "        action[7] = 1\n",
    "    else:\n",
    "        action[0] = np.argmax(Q_tire[(current_state[0],current_state[0+8])])\n",
    "        action[2] = np.argmax(Q_wheel[(current_state[2],current_state[2+8])])\n",
    "        action[4] = np.argmax(Q_motor[(current_state[4],current_state[4+8])])\n",
    "        action[2] = np.argmax(Q_gears[(current_state[7],current_state[7+8])])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 700 :  33\n",
      " 725 :  28\n",
      " 695 :  20\n",
      " 710 :  80\n",
      " 705 :  58\n",
      " 720 :  40\n",
      " 715 :  61\n",
      " 690 :  15\n",
      " 670 :  1\n",
      " 685 :  3\n",
      " 680 :  4\n",
      " 730 :  2\n",
      " 675 :  2\n",
      " 665 :  2\n",
      " 660 :  1\n"
     ]
    }
   ],
   "source": [
    "#tire\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[0,2]] = max(ac(current_state)[[0,2]])\n",
    "    if action[0] ==1:\n",
    "        time_replace.append(current_state[0])\n",
    "        #action[[0,2]] = 0\n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[0] ,action[0],r[0],obs[0] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 340 :  724\n"
     ]
    }
   ],
   "source": [
    "#wheel rim \n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action = np.zeros(8)\n",
    "    action[[4,2]] = max(ac(current_state)[[4,2]])\n",
    "    if action[2] ==1 and current_state[2] >= 300:\n",
    "        time_replace.append(current_state[2])\n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[2] ,action[[2,4]],r[2],obs[2] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 340 :  712\n",
      " 335 :  12\n"
     ]
    }
   ],
   "source": [
    "#motor\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[4,2]] = max(ac(current_state)[[4,2]])\n",
    "    if action[4] ==1:\n",
    "        time_replace.append(current_state[4])\n",
    "        \n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[4] ,action[4],r[4],obs[4] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)\n",
    "#time_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 700 :  354\n"
     ]
    }
   ],
   "source": [
    "#shifting gears\n",
    "time_replace = []\n",
    "current_state = reset()\n",
    "action = np.zeros(8)\n",
    "for j in range(50000):\n",
    "    action[[7,2]] = max(ac(current_state)[[7,2]])\n",
    "    if action[7] ==1:\n",
    "        time_replace.append(current_state[7])\n",
    "        \n",
    "    obs , r = env(action,list(current_state))\n",
    "    #print (current_state[7] ,action[7],r[7],obs[7] )\n",
    "    current_state = obs\n",
    "CountFrequency(time_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
