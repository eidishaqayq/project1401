{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning for policyIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    st= [0]*17\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(action,st,step,i): \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    reward =[]\n",
    "    \n",
    "    if step == 5000:#scheduled overhaul \n",
    "        for j in range(17):\n",
    "            st[j] = 0\n",
    "        st[8]=1 \n",
    "        reward= -(time_interval / tp[i])*time_interval * math.ceil(0.8 *sum(tp)/time_interval) \n",
    "    else :    \n",
    "        if action == 0 :\n",
    "            if f <= st[i]:\n",
    "                st[i+9]=1 #fail\n",
    "                reward = -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "                \n",
    "            else:\n",
    "                #st[i+9]=0\n",
    "                st[i] +=5 #age\n",
    "                reward = 5\n",
    "        \n",
    "        if action ==1 :\n",
    "            if st[i+9]==0 :\n",
    "                reward= -(time_interval / tp[i])*tp[i]\n",
    "            else:\n",
    "                reward= -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "            st[i]=0\n",
    "            st[i+9]=0\n",
    "            \n",
    "            \n",
    "    \n",
    "    return tuple(st) , reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env1 (action,st,step,i): \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    reward =[]\n",
    "    \n",
    "    if step == 5000:#scheduled overhaul \n",
    "        for j in range(17):\n",
    "            st[j] = 0\n",
    "        st[8]=1 \n",
    "        reward= -(time_interval / tp[i])*time_interval * math.ceil(0.8 *sum(tp)/time_interval) \n",
    "    else :    \n",
    "        if action == 0 :\n",
    "            if f <= st[i]:\n",
    "                st[i+9]=1 #fail\n",
    "                st[i] = 0 #replace due to failur\n",
    "                reward = -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "                \n",
    "            else:\n",
    "                st[i+9]=0\n",
    "                st[i] +=5 #age\n",
    "                reward = 5\n",
    "        \n",
    "        if action ==1 :#PM action \n",
    "            reward= -(time_interval / tp[i])*tp[i]\n",
    "            st[i]=0\n",
    "            st[i+9]=0\n",
    "               \n",
    "    \n",
    "    return tuple(st) , reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "min_lr=0.1\n",
    "discount=0.5\n",
    "decay=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon, state,i):\n",
    "    if (state[i+9]==1):\n",
    "        return 1\n",
    "    else:\n",
    "        if (np.random.random() < epsilon):\n",
    "            return random.choice([0,1]) \n",
    "        else:\n",
    "            st=(state[i],state[i+9])\n",
    "            return np.argmax(Q_table[st])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate( t):\n",
    "    \n",
    "    \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "    # Learning rate also declines as we add more episodes\n",
    "    return max(min_lr, min(1., 1. - math.log10((t + 1) / decay)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replacement time for tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=0\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c9zq3rPTjqQDYISZJO1RdGRQcEB1yCiRETRYcyoDMiM/hTQn+KM44ALmyj+UEQQZROVyCqLiKMsSQSBEJCwJSEh6ezpdNLdVff5/XFOdVd3ujudpG9Xlu87r0rdOnep596uuk+dc+5i7o6IiMhgSyodgIiI7JyUYEREJBNKMCIikgklGBERyYQSjIiIZEIJRkREMqEEI5IhM3ubmT1vZi1mdmKl4xEZSqbzYESyY2b3AzPd/bJKxyIy1FSDEcmAmeXj4F7A3G1chsgOSQlGpIyZuZmdbWYvmtlyM/uOmSVl4//ZzOaZ2Sozu8fM9uox75lm9jzwvJm9ALwO+F1sIqsxswlmNtPMVprZfDP7dNn8F5jZr8zsejNbC3zSzB40s2+a2V/iMn5nZruZ2S/MbK2ZzTKzKWXLuMzMFsZxc8zs7T2Wf7OZXWdm68xsrpk1lY2fbGa/NrNmM1thZlcMZL1F+qIEI7KpDwJNwOHANOCfAWIfyvnASUAj8Cfghh7zngi8GTjA3V8PLADe7+7D3L0tTr8ImACcDHzLzI4tm38a8CtgFPCLWDYd+DgwEXg98DBwDTAGmAd8vWz+WcChcdwvgVvMrLZs/AeAG+PyZwJXxHXLAbcDrwBT4nvduAXrLbIpd9dDDz3iA3DghLLXnwPuj8N3AWeUjUuAVmCvsnnf2WN5LwPHxeHJQBEYXjb+f4CfxeELgId6zP8g8JWy198D7ip7/X7giX7WZxVwSNny7ysbdwCwIQ4fBTQD+V6W0e9666FHXw/VYEQ2tbBs+BVCbQNCf8plZrbazFYDKwEj/Nrvbd6eJgAr3X1dj+Vvbv6lZcMbenk9rPTCzL4Qm7LWxBhHAmPLpn+tbLgVqI19PZOBV9y90Mv7D2S9RTahBCOyqcllw3sCi+PwQuBf3X1U2aPO3f9SNn1/h2UuBsaY2fAey391gPP3K/a3fBn4CDDa3UcBawjJYHMWAnv2cWDBQNZbZBNKMCKb+j9mNtrMJgOfB26K5T8CzjOzAwHMbKSZfXigC3X3hcBfgP8xs1ozOxg4g66+lm01HCgQm7rM7GvAiAHO+xiwBLjQzBpifG+L47ZpvWXXpQQjsqnbgDnAE8AdwNUA7v4b4CLgxniU19PAu7dw2R8ldKIvBn4DfN3d7x2csLmH0F/yd0LT20b6b7Lr5O5FQn/OPoQDExYBp8Rxg7HesgvSiZYiZczMganuPr/SsYjs6FSDERGRTCjBiIhIJtREJiIimVANRkREMqGL6UVjx471KVOmVDoMEZEdypw5c5a7e2Nv45RgoilTpjB79uxKhyEiskMxs1f6GqcmMhERyYQSjIiIZEIJRkREMqEEIyIimVCCERGRTCjBiIhIJpRgREQkE5mdB2NmPwXeByxz94Ni2RjCvTWmEG4l+xF3XxXHnUe4N0YRONvd74nlRwA/A+qAO4HPu7ubWQ1wHXAEsAI4xd1fjvOcDnw1hvJNd782q/WUfqQptLdDbW0YvvtuaG3t/jjkEDj22DDdd74DpUsXmYX53vpWOOooaGuDO+4IZQ0NMGpUeDQ2Qn19ZddzJ+LumNlmy/obl6YpSZL0OV1/71F6TtOUcGHrpLM8XNYqxSzXbVr3lNI91cqXG5ZhndMCeJqCOWa5zrIwvkD3e605UMQ9h1npY1kkLD4POO50xlhaTpoWSZIcadoBWHyfFGgHz2FJDvcYo68Hqwc6MKuJ8bSXfQcAr8aSFE89FnSA1QJFKC2HdYRbAXksB6wFGBmH28L7+IYYu4GlQDVmYdu5p53r39ffemtkdi0yMzsaaAGuK0sw3ybcMvZCMzuXcNe9L5vZAcANwJGE28reB+zr7kUze4xw06dHCAnmcne/y8w+Bxzs7p8xs+nAB939lJjEZgNNhM07BziilMj60tTU5DrRsm+97hRaW7E1a2D8+PD682fBCy9iryzAly2DFSvwT52GXXU1RoLncliPz5ufczb+vUuwluXYyN03ed/0a1/Av3Q0/tpy8vucsen4/z4b/+IX4O93kbznPBg9BsbUwthaGFtNeup0/NBR2Lp1MO85GNsB498CDcOxjrmQjIHCk5DbE3KToP0poAp8CdgYSKogNUiqwVeAF8BrgPbwRU/XAeshGQe+GqwaistDcDYWrAPSBHxR+JIn43GrA1+NFV4Aa4RkREiohYXgrZAbGXcyCfg6oACeA0vC8r0jPJI8pK1AHEcSY2sB2oDq8LCqEDcbS1stTEsHUBtft8VxHWF5VBP3cOH9SeP71MUdW2l5KVAfJ20rm746vofHaQpxuRZfE8endL+JZ/l75kqfkrL5SrHXxFhL8+djebFsGaX5SssgLjeJy067T+MGqwqw3GFNBxxRFyZ9ZD3MacPWFWBdCmtT2JjiP54Y5v/+KuzOddDu0JaGxTYk+L17AYb91zL4w3rIG9QnUG+wRxV+8bjwvre3wPIijM3DGAvPE/IwLP5NO/9m5duo9JyWDfe2Ly+VJ3RfBkAV4e8PnX/bZBg2+kosv2cvy+qdmc1x96bexmVWg3H3h8xsSo/iacAxcfha4EHCLV6nATe6exvwkpnNB440s5eBEe7+MICZXQecSLip0jTggrisXwFXWNgDHg/c6+4r4zz3AicQEtguzd3jr6a6WBI+vGnHfMIXcyPk9iVhLYUNd2OeQvsf8KqDsbYH4L4F2Jxh2DMLsefWwcstpO8YRvGXbwUbQf6B28PyJtbih+ax3UbhR9xDuvRAsAlwxxSoLUBdAvXVUOdQeycsuxM8hQWvC2GVvjftDsmvYd1vocop3DcJ2hxbn8Jah3VF/JDfwMo7oK2Av7kD1i7BVqbwchGWF+CNL8DU4fijG8ifVLrz8Y/weoPGPOl3xsHb67D5HTBzHeyeh3E5aIzP4/JhxzBYf4PiMziOYWHX5q/haXjdqdjcx8x034ekPcZtorWfcT2m6aZAV8LpGUNv07f38h6FXqbr6C+QXvTcIZYU2TTu9r4Xsz6FhR3QXITmAjQXseVF/KzRMCIHP1uNXbYSlhexsrDTZ18HI3PYfS3YD1bhCTAigeFJSBRpERKDGofRCVQb1FjMz11Jzhtz2F5VUHBodVhZDMORXbca+9OGbiH766rwP08J2+DiFdgGx/ethv2qYZ/q8B3qttH7+iOXynvblh10+5v4akjrwj5ikAz1pWJ2d/clAO6+xMxiCmcioYZSsiiWdcThnuWleRbGZRXMbA2wW3l5L/N0Y2YzgBkAe+458Iy9PXJ3ih1/xzseJal+J5bUU2i9lXTD3XgS7pqbmOHtc4B2DCfFMPLA+s7dW4pjJNjLbSQPbSR5rUjxS38mwchdvhibtRHfu4p0/yo4cRR+eC1efB6A4r1hMxc9LDnprPEU8PQV7LCE8Ms2vFM3ZuEHVblq6z58YE1Y1942wMQ86fe7akBhB+5dzQ37VVP8+R7hl+LyItZcDDuaMbmwG5jXRu47KzdZbDpzErypDn7fgv14dUg443L4uDw05uD4BhieC8kwT9jh9KOUSLqSTI/kIptXjDvpYUnY0b7UDvesj3/TkEBYXsSv2B3eUAO3riP58rJui/Aq4CMjQoKZmIdjGmBsjrQx/rgYlUBt/Fv9+xj838eEmkdvzUczRuMzRvcd72dG45/pe7z/YiK+KsTMirgOua73sUc3wMMbSGIu8AQ4cTj+gz1CwXNtsHd19+/LVqnDxtyAVe2/jcvpsr1ci6y3LeP9lG/tPN0L3a8CroLQRLb5MCsnLS4myU0AwNP1uLdQaHuEwvprcd+AeQeWvgwUcb4OlG2IYlxGfFleme7aYAazN1B163qSP24geTn8lPOJOew/RpHmwS8fi4/JhV9vURKX5kDqKYklJBgpTup0JpnBbNcdiM4dt8VEMzqHH9dQNj6w+I/3Dyd9uSF8yZuLsKwAy4rw+pgQi0Cbw5yNsLRAsjEsIZ01JSSYH63CvrMi7Jwac521H//PRmhI4O/tsKoI43LYuDw0hLgSHWcTbEw7kz8r4nNTbdj+89qwC5aHGmlzGG8ppNeNh3cNg/ntJN9YjlcbjI3bfny+6498dB3pj/YIf5exsWY6MulKFu8ahr9rWN+xNWT8N6qy+MOl992x3zQJOhx/qQOea8OebccnxGnbUuz4heFLfHANfnQ9HFMPh9Z2S1IDYSP/C6s6YBtXpruhTjBLzWx8rL2MB0o/KxYBk8umm0S4Z/miONyzvHyeRRZ6p0YCK2P5MT3meXBwVyM7pY5MTzfQ0XoN+br30956Jx0tl2C5CZCMwwvzwNeGXZNZjxZn6/r13scv49KYpNWx+1opHlMHIxLyc9rJ3dJC+tZa2v9lBP6PdfC6KswsJI1JVZ3JoyTFyVmCu3eWmxmJE19nl1i61rP7cM9pwjpvOt0mNYeaBCYmMLFnVQp49zD83XEn5I6v95CE9ohfoTfVwedGdyWnpUWY2975q9KuWY39bE3n4rzeYHwe/9NeYUf3m3Xhl/i4sia6UUn4ZbojST30U6xOwy/+xjy0pnDLWliTYmtSWF2ElUX8Q8PhfcPhuTaSYxZsuqiLxoUEkwAtKUyugsNqQ9NmYy7UTgDeXh+as0YkvdcwplSHx46symDfati3Gn9/91H+/d2x2RvhsQ3Y91Zi311Jev5ucNaYULNuTWFUrvflli9n7QWQ3x+rmjpoYWd6w7HYB3N7WSf/d4AVZZ38Y9z9S2Z2IPBLujr57yfcF71oZrOAs4BHCZ3833f3O83sTOCNZZ38J7n7R2In/xzg8BjGXwmd/Ju2f5TZHjr508KrtKw8nbS4BqyIpSviL/Gga8dY2lF2dYOW1xD6/VXsTvJIG7kb1pG7vRXb6LRfMZbiScNIWj386qmx2FQW3ifpsSNOy+IASGKCcbpqLOVH6GShr4RSPr6kr+nKp8m8mWphB7zYHmpFzaF2ZBuc9KJwlfPkc69hv2npNotPzOOz9w7xfWIxzNoQ2v+HJ6F5aP8a/MLYyvz9ldhrhdAHUJPgNQZ75uGk0DzKbetC53RC6QMUOpL/saFr/LoUOrzr8fpqOCEkVfuf5WEnvz6F9WGn5f9YDzNGQ8GxN70Uyten4QAlwM8cjX91LKwpkuz3YiirsZA4R+Xwz4yC6SNhXRF+uibUPkq1jFJNpFY1vC2ysgh/aoWDa8KPk7tbsBlL4Oh6/P3D4YQGGNlPsrFh2JibtijJVKST38xuINQkxprZIuDrwIXAzWZ2BrAA+DCAu881s5uBZwi9g2e6e2zY4bN0HaZ8V3wAXA38PB4QsBKYHpe10sz+C5gVp/vPzSWXSnJ3ioX5rG/5CV58DYqLcG8h9VjLKHUGl88T/zcsHNsz0J34qiI171tC8lIBH26kHx5G4cQG0iNrwvLrk7JlB0lZP4GX7aq7jmOxbsklDIfpyw8PDYdsbtUm6tVAkkt/0/S+vAzTzOQqmFwVt2Fpe5Zqn076wz3gEsdKTXTNhW6/xv3YemxSPiSB8kdkf9kAf9sIGx02OomDH1WHxwRj31mBvdC9k92PrcdjgrELmrHXit3Hf2AYHhMMN6wNSac+CU1GDRZ+HUM4COK4htAfMiwhHRkSSKnPjBEJ6eN7h2apul4SxvAcfH7MADZiX0dKbYnejqYaqPL331wsvX2SSkevlObN0/0outI3PUdXm0TpUfplQNk8pY7+0hFxwJgEpo2KZQWYWgufHg0z15HcvxSvNXjvcPxbjaH/KdkjHhmYhgM4kmqwzdd2Bkq3TI4qUYMpFleyeuVZePoaXnyW8AGqIk8RSDs/iqUaRNJjJ18qL08wnX0KpdfzO0iebKN40jAMyJ+7grSphvQ99XjsS+mqEXVvWCuvKfVWY+hqjks7X2NGGjv5DTDL4xSJJw30sQMvP5RyGOHL0UrnziD3eqAIxRcIh6ca5N8I6QuQrgTykIyH3EQoLgEbAcUVUL0/FBZBLnawpq2QnwqF+eHLlJ+CFZdA9dvAX4XCAix/OJh3HXbsxfA6bQvLtVrCuQg14BvBGiAZDekisHHgayE3OaxDcQHhPIcNkD8I0hWQvooXl0MyEqt+JxQeh7QV91WhczW/P7TPg2QY+AbITwRKR/YkYfkMg6QefA3YHmDtXYcu+xqwsVAcBe3N4WcZBWheBYW6uL/KQVqAuhEwthDWYXFrSGjVwyFZCTUjwvEY+frwN0hqwzqlK8POyHLh8GpqIG0mHEI9inD02YgQB4Uw3jy8h5eOTquJf9uNXUc0egedh1MbYMPAW+g8jNrbw6/rJMHTNkiXgI2P75uEw8XTdZAbR+ehzN4Blg+fHaroPDzZ2sPf0OvidOshqYE0HmGZrgIbDkncTkkNFFdg+fHxvVdDrjF+puNnw9dDbrfw2fQ2sNgkF79QoSYfzrfBQ0JIclWkaTi3xtMU3LBcClR1nt8Tzttpw6y+23k/ZkXcu85bCWXh++zuJElC+b7dCyuwOfOxa6+Fxx4jffRhoIXk4bnwxjfCqFGd5xNtaatDfzUYJZhoqBKMu5Omy1i//jbWr7sYowPYQM5GAmvDZxanynKbHGGU9EgCofmqS2KlhGHYU23kL15N/p4N+MiEjXMmdXbOlzr1y+sjRjVGDSmtQAFLJmO58ZDsCR0P495OUvdJSBdAfiLmtVh+KlTtE45M65hD0nA60IGnreAtGDXkat9MsbA0TJObhOUPjDvLFow2SCbgniOxDTi1JElV3E6lE8CKJEn4Inm6Hksaup+0l7aC1W3RyYEiu7T4Y4+2NpgwITz/9a+w775btbiKNJHJptyLNK/4V9rafo9RJClrmCn6mlAJNiMfx/RMLuV9H92TS6gBpJ5irxSp/q/V5O9qxUcldHzxQIqffjuMaAeWkq99bzip0FdC1QlQmEVStS9JbiyWjCJNC3i6klx+XHnouLcRLp7Qi6pJhNOSepfL7w7595SVjIiPcsO71W5Kv8bKz662JDbnlNfYkt7P4ldyEelD6btRUwP33Qe33AJTB69jv5wSzBAqFJcBtaTeQWKQdvawdPV55GMJdG+qSns0T4VjzRpijcNJGEe+tomkysg9ehN+wRfh7H+lavSETU4v6ab6Pd1eJkk+NA/10GdyEZEd12GHhUdGlGCGQOptrFr7/1i57ntU5w+grvaDbNj4GxJLN6mNdHX3ebyohZFQCzSQ4lRVvRGsSC43ldqR/xdaluAXfplkQQd2/Y9gDPDqJeGaXSIiFaQEk7HUN7Jg6Qdp63iSnNXSXvgbHi/PUOrAL2/M6XDDKZKzUcBailRTO+Ib1DV8BLy9e5PQrbfCOefAokVwyinhgpHV1UouIrJd0EHmGUuLRaryUwGn6BswavHCXHLEo8DiYb0dhGNuihSBeoreSsOI7zK68V7qh52KWb4ruSxbBh/6EJx8MowdCw89BDfeGJKLiMh2QjWYDLVsfJiXm2eQWC0j606iZcOtJL4hXMHBIHUnlzuU9uITJBauAVadfwPDhn+ZmpojyOXG9r5gM3jsMbjwQvjCFyCvP6OIbH+0Z8qIu7Nm40MUvZWir2XNxkfC6VMWLw3m8aLm5tTVnQS+hqr8VEaN/EqP+1JEy5fDZZfBBReEe6A8/7yawkRku6YEk5G1G//C4rVXMrz6cNranyBJF5Mk4VDjosfLvFgd7YUnMatm0rhbSPo6UuuBB+C002DFCnjf++DNb1ZyEZHtnvpgMlBMW3h59YUMq3k769rnUG3hEhypQ8GN+toPhul8A9X5g6itfiNGL/0nhQKcfz4cdxyMHAmPPhqSi4jIDkA1mAwU0lZq83uzovV2RuT2pZg+g2EUPFxnaG3bY4yq/yj53GgaR34Jsz465884A667Ljxfdlm4VbCIyA5CCSYDzy7/IuvbX2Bk9RF0tD8CllDAgDqqKNCRLmNN2yz2G/9A7/0tJWefDe98J5x++pDFLiIyWNREloHXjT4PKNLS8SRu0O5Gh+dx2qipPoS66kOZMOprvSeXm28OzWIARxyh5CIiOywlmEG2pOUO/tb8f5g8/BOYt9HmCR0ebstbX30U69r/ym7DZzCy/rjuM7rDRReFEyYfegg2bqxI/CIig0UJZpDV5sezsbCEBWsvo+BGu1dRIE+SjGdN26Ps1nAauzW8t/tMxWJoDjv3XPjoR+H++3WUmIjs8JRgBtno2sM5aOy3SL1Ih+dp83y4DL47jQ0fZtywUzad6fTT4YorwkmT118frnIqIrKDU4IZZKva5vFay/U4NbSRJyVhr9FfAZy17c/SUH3QpjOdeCJcfDF897uQ6E8iIjsHHUU2iF5b/wh/eu1MJtcdykZPySej6EhbmLvim7xh9BfYo+HtXfcpWbs2nNfyrneFa4qJiOxk9HN5kBTSjTyz+gZGV01gTdvD1ORfz7hhp7E+zZGzhMUtt1CV2y1MvGYN/NM/wbRp8NprlQ1cRCQjqsEMEiel6BtY1b6UUflGlrUvZln7T0m9ir1Gfo5i+ir5ZASsWgXHHw9PPBHuJLfHHpUOXUQkE0owg6QqqeeYCZfy4OJzWLHxcXKxKWzPYcex35gZYaJ16+CEE+Bvf4Nf/zpcV0xEZCelJrJBVJXUs3vdweQsJXWjNjeeRet/zzOrfhImuP56mDMn1FyUXERkJ6cazCApeoFX1z/Ic6uvIXWjSEJLYQW71x7K3JVXMrp6P8Z/5jPw1rfCIYdUOlwRkcypBjMI3J3fv/p1Hl56EUXPc9TuF/LBKb8nsWG8tuEp/uHS4ezx8qhwozAlFxHZRSjBDAIzY0L9oWxM11NbfTB71B/FfUu+xYpCgTddtp7xV/wRm/m7SocpIjKk1EQ2SA4Z82GMHP/b/H1+Mj/0r5x4/6FM+MGl8OlPh8vAiIjsQlSDGUQHjOrquJ/w2GrGn/MDOPZY+MEPQvOYiMguRAlmkBTSdu569auA0VgzlYN/voj1e42geMsNUFVV6fBERIacEswgcHfuXvw1FrbO5h17fJEPT7mK1ut+wK1X78u9rVfg7pUOUURkyKkPZhCYGfsMfwevb/gH9r90Fpx5JAft8RG8ppq6/Kiu64+JiOxCVIMZBM+tncWsVbPY9/uz4IILYOZMnlr9v8xd9zxTGt5W6fBERCqiIgnGzP7dzOaa2dNmdoOZ1ZrZGDO718yej8+jy6Y/z8zmm9lzZnZ8WfkRZvZUHHe5xaqCmdWY2U2x/FEzm5Ll+rQWW/C77yK54D9JP/ZRnvrgVG5ZeAnrC6tJKWb51iIi260hTzBmNhE4G2hy94OAHDAdOBe4392nAvfH15jZAXH8gcAJwA/NLBcXdyUwA5gaHyfE8jOAVe6+D3AJcFGW63TY2tdx6vlPsXSfEXzz7HXcvOhi9mrYj9P2+irVie5MKSK7pko1keWBOjPLA/XAYmAacG0cfy1wYhyeBtzo7m3u/hIwHzjSzMYDI9z9YQ+96Nf1mKe0rF8Bx1qWHSHnnEO+aDz8o0/RURe6tabv+SVqcnWZvaWIyPZuyBOMu78KfBdYACwB1rj774Hd3X1JnGYJMC7OMhFYWLaIRbFsYhzuWd5tHncvAGuA3XrGYmYzzGy2mc1ubm7e+pW66ipeuOFbPN7YFeYtCy+hI23b+mWKiOzgKtFENppQw9gbmAA0mNlp/c3SS5n3U97fPN0L3K9y9yZ3b2psbOw/8H48VfUc105+gL0a9uOrB/ySkyadxYstT/KLV/5HSUZEdlmVOEz5OOAld28GMLNfA28FlprZeHdfEpu/lsXpFwGTy+afRGhSWxSHe5aXz7MoNsONBFZmtD4klmPvhoM4da9zqcnVcdjodwIwd83D9J7rRER2fpXog1kAvMXM6mO/yLHAPGAmcHqc5nTgtjg8E5gejwzbm9CZ/1hsRltnZm+Jy/lEj3lKyzoZeMAzPNvxwJFH8cm9L+jW53LY6Hfysb3OpyqpzuptRUS2a5Xog3mU0PH+V+CpGMNVwIXAu8zseeBd8TXuPhe4GXgGuBs4091Lx/5+FvgJoeP/BeCuWH41sJuZzQf+g3hEWhaKXuSh5gfxsha4jrSDh5r/mNVbiojsEEyXMQmampp89uzZWzzfnFWzufKFK3jrbm/jk1POoOhFfvTCD/jbmif40hvOY9/hb8ggWhGR7YOZzXH3pt7G6VIx2+iI0U18YMKJzFz8WwpeZEOxlafWPMnH9vy4kouI7NKUYAbBByacSCEtcOdrtwNw6p6n8Y5xx1Y4KhGRytK1yAZBR9rBqxu6Tsl5ef1LpJ5WMCIRkcpTDWYbdaQdnX0uH9vz46wrrGPm4t8C8MkpZ5CYcriI7JqUYLbR02ue6kwu5c1iv1t8G28fewxTh0+tYHQiIpWjo8iirT2KDGBR60Im1U/ebJmIyM6mv6PI1H4zCHpLJEouIrKrU4IREZFMKMGIiEgmlGBERCQTSjAiIpIJJRgREcmEEoyIiGRCCUZERDKhBCMiIplQghERkUwowWyjlW0t9Lzczsq2lgpFIyKy/VCC2QZLN67h1D9fxo+ev7czycxb8yof/tPF/HrBoxWOTkSksnQ15W3QWDOco8cdwDUvPgjAMbsfyL/NupphVXW8Zey+lQ1ORKTClGC2QWIJ5x44DYBrXnyQa158kPG1o7jyTf/ChPrRFY5ORKSy1ES2jRJLOHHymzpfHzpmCuPrRlUwIhGR7YMSzDaat+ZVzpr1U/aoHcWbd5vKXYuf6NYnIyKyq1IT2TZYumF1Z5/LlW/6F/aoG8mFc2/jmhcfZHR1A9OnvK3SIYqIVIwSzDYYVzuSj+99NP80/pDOPpdzD5zGuNoRHDf+4ApHJyJSWUow28DM+OTrj+lWlljCv+xzbGUCEhHZjqgPRkREMqEEIyIimVCCERGRTCjBiIhIJpRgREQkExVJMGY2ysx+ZWbPmtk8MzvKzMaY2b1m9nx8Hl02/XlmNt/MnjOz48vKjzCzp4ZsSywAABLESURBVOK4y83MYnmNmd0Uyx81sylDv5YiIru2StVgLgPudvf9gEOAecC5wP3uPhW4P77GzA4ApgMHAicAPzSzXFzOlcAMYGp8nBDLzwBWufs+wCXARUOxUiIi0mXIE4yZjQCOBq4GcPd2d18NTAOujZNdC5wYh6cBN7p7m7u/BMwHjjSz8cAId3/Yw3VZrusxT2lZvwKOLdVuRERkaFSiBvM6oBm4xsweN7OfmFkDsLu7LwGIz+Pi9BOBhWXzL4plE+Nwz/Ju87h7AVgD7NYzEDObYWazzWx2c3PzYK2fiIhQmQSTBw4HrnT3w4D1xOawPvRW8/B+yvubp3uB+1Xu3uTuTY2Njf1HLSIiW6QSCWYRsMjdS7d8/BUh4SyNzV7E52Vl008um38SsDiWT+qlvNs8ZpYHRgIrB31NRESkT0OeYNz9NWChmb0hFh0LPAPMBE6PZacDt8XhmcD0eGTY3oTO/MdiM9o6M3tL7F/5RI95Sss6GXjAdf18EZEhVamLXZ4F/MLMqoEXgU8Rkt3NZnYGsAD4MIC7zzWzmwlJqACc6e7FuJzPAj8D6oC74gPCAQQ/N7P5hJrL9KFYKRER6WL6YR80NTX57NmzKx2GiMgOxczmuHtTb+N0Jr+IiGRiwAnGzOrK+k1ERET6NaAEY2bvB54A7o6vDzWzmVkGJiIiO7aB1mAuAI4EVgO4+xPAlGxCEhGRncFAE0zB3ddkGomIiOxUBnqY8tNmdiqQM7OpwNnAX7ILS0REdnQDrcGcRbiacRvwS8K1vc7JKigREdnxbbYGEy+NP9PdjwO+kn1IIiKyM9hsDSaeNd9qZiOHIB4REdlJDLQPZiPwlJndS7j6MQDufnYmUYmIyA5voAnmjvgQEREZkAElGHe/Nl6Yct9Y9Jy7d2QX1o7hvhdeYFh1NW+Z3HU3gd899yx7jhzFIXvsUcHIREQqb6Bn8h8DPA/8APgh8HczOzrDuLZ7xTTl0kf+whm3/YZHFoYbbv722Xn8+913ceWsxyocnYhI5Q3oaspmNgc41d2fi6/3BW5w9yMyjm/IbM3VlJvXr+e0W3/ForVrmLbf/tw892nePHESP552IvVVVRlFKiKy/RiMqylXlZILgLv/Hdjl96CNDQ1c/6GT2VAocOPTT+HuSi4iItFAE8xsM7vazI6Jjx8Dc7IMbEfx54ULOocdePK11yoXjIjIdmSgCeazwFzCJWI+T7i75GeyCmpH8dtn5/HFe+7mqEmT+eOnzmDqmN269cmIiOzKBppg8sBl7n6Su38QuBzIZRfW9q+Yplz3xOOdfS6TR47k+g+dzKQRI/nlU09WOjwRkYobaCf/I8Bx7t4SXw8Dfu/ub804viGzNZ38azZupCqX69bnsnLDBhqqqqjJD/QUIxGRHVd/nfwD3QvWlpILgLu3mFn9oES3AxtZW7tJ2Zi6ugpEIiKy/RloE9l6Mzu89MLMmoAN2YQkIiI7g4HWYM4BbjGzxYSDpSYAp2QWlYiI7PD6rcGY2ZvMbA93nwXsB9wEFIC7gZeGID4REdlBba6J7P8B7XH4KOB8wuViVgFXZRiXiIjs4DbXRJZz95Vx+BTgKne/FbjVzJ7INjQREdmRba4GkzOzUhI6FnigbJyOwxURkT5tLkncAPzRzJYTjhr7E4CZ7QOsyTg2ERHZgfWbYNz9v83sfmA84cTK0lmZCXBW1sGJiMiOa7PNXO7+SC9lf88mHBER2VkM9ERLERGRLVKxBGNmOTN73Mxuj6/HmNm9ZvZ8fB5dNu15ZjbfzJ4zs+PLyo8ws6fiuMvNzGJ5jZndFMsfNbMpQ71+IiK7ukrWYD4PzCt7fS5wv7tPBe6PrzGzA4DpwIHACcAPzax0JecrgRnA1Pg4IZafAaxy932AS4CLsl0VERHpqSIJxswmAe8FflJWPA24Ng5fC5xYVn6ju7e5+0vAfOBIMxsPjHD3h+PBB9f1mKe0rF8Bx5ZqNyIiMjQqVYO5FPgSkJaV7e7uSwDi87hYPhEov4PXolg2MQ73LO82j7sXCIdU79YzCDObYWazzWx2c3Pztq6TiIiUGfIEY2bvA5a5+0BvudxbzcP7Ke9vnu4F7le5e5O7NzU2Ng4wHBERGYhKnI3/NuADZvYeoBYYYWbXA0vNbLy7L4nNX8vi9IuAyWXzTwIWx/JJvZSXz7MoXolgJLASEREZMkNeg3H389x9krtPIXTeP+DupwEzgdPjZKcDt8XhmcD0eGTY3oTO/MdiM9o6M3tL7F/5RI95Sss6Ob7H5m/dKSIig2Z7up7YhcDNZnYGsAD4MIC7zzWzm4FnCLcKONPdi3GezwI/A+qAu+ID4Grg52Y2n1BzmT5UKyEiIoHph33Q1NTks2fPrnQYIiI7FDOb4+5NvY3TmfwiIpIJJRgREcmEEoyIiGRCCUZERDKhBCMiIplQghERkUwowYiISCaUYEREJBNKMCIikgklGBERyYQSjIiIZEIJRkREMqEEIyIimVCCERGRTCjBiIhIJpRgREQkE0owIiKSCSUYERHJhBKMiIhkQglGREQyoQQjIiKZUIIREZFMKMGIiEgmlGBERCQTSjAiIpIJJRgREcmEEoyIiGRCCUZERDKhBCMiIplQghERkUwowYiISCaGPMGY2WQz+4OZzTOzuWb2+Vg+xszuNbPn4/PosnnOM7P5ZvacmR1fVn6EmT0Vx11uZhbLa8zsplj+qJlNGer1FBHZ1VWiBlMAvuDu+wNvAc40swOAc4H73X0qcH98TRw3HTgQOAH4oZnl4rKuBGYAU+PjhFh+BrDK3fcBLgEuGooVExGRLkOeYNx9ibv/NQ6vA+YBE4FpwLVxsmuBE+PwNOBGd29z95eA+cCRZjYeGOHuD7u7A9f1mKe0rF8Bx5ZqNyIiMjQq2gcTm64OAx4Fdnf3JRCSEDAuTjYRWFg226JYNjEO9yzvNo+7F4A1wG69vP8MM5ttZrObm5sHZ6VERASoYIIxs2HArcA57r62v0l7KfN+yvubp3uB+1Xu3uTuTY2NjZsLWUREtkBFEoyZVRGSyy/c/dexeGls9iI+L4vli4DJZbNPAhbH8km9lHebx8zywEhg5eCviYiI9KUSR5EZcDUwz90vLhs1Ezg9Dp8O3FZWPj0eGbY3oTP/sdiMts7M3hKX+Yke85SWdTLwQOynERGRIZKvwHu+Dfg48JSZPRHLzgcuBG42szOABcCHAdx9rpndDDxDOALtTHcvxvk+C/wMqAPuig8ICeznZjafUHOZnvVKiYhId6Yf9kFTU5PPnj270mGIiOxQzGyOuzf1Nk5n8ouISCaUYEREJBNKMCIikgklGBERyYQSjIiIZEIJRkREMqEEIyIimVCCERGRTCjBiIhIJpRgREQkE0owIiKSCSUYERHJhBKMiIhkQglGREQyoQQjIiKZUIIREZFMKMGIiEgmlGBERCQTSjAiIpIJJRgREcmEEoyIiGRCCUZERDKhBCMiIplQghERkUwowYiISCaUYEREJBNKMCIikgklGBERyYQSzFZatXQ1j94xp1vZsgXN/PW+JysUkYjI9kUJZiv99Pxf8rUTv80fb3kYCMnli+/8Bhd+/HI2rN9Y4ehERCovX+kAsmRmJwCXATngJ+5+4WAt+7OXfopFzy/hW6deSvPC5cz84T2sXbGOC+/5KnUNtYP1NiIiOyxz90rHkAkzywF/B94FLAJmAR9192d6m76pqclnz569Re/Rum4DMw7+AktfaQbg+498i/2OnLpNcYuI7EjMbI67N/U2bmduIjsSmO/uL7p7O3AjMG0w36BlVQttrW2dr5e+snwwFy8iskPbmRPMRGBh2etFsayTmc0ws9lmNru5uXmLFl7qc+loL/Dt+77GQf+wH9869dLOPhkRkV3dzpxgrJeybu2B7n6Vuze5e1NjY+MWLfwX37y1s8/lsHe+kf++43wOOGpffnjONerkFxFh5+7kXwRMLns9CVg8WAs/8/J/5sSz38PeB+0JQP3wOv77jvNZsXilOvlFRNi5azCzgKlmtreZVQPTgZmDtfDq2urO5FJSP7yOyW+Y2MccIiK7lp22BuPuBTP7N+AewmHKP3X3uRUOS0Rkl7HTJhgAd78TuLPScYiI7Ip25iYyERGpICUYERHJhBKMiIhkYqe9VMyWMrNm4JWtnH0ssD2exq+4tozi2nLba2yKa8tsS1x7uXuvJxIqwQwCM5vd17V4KklxbRnFteW219gU15bJKi41kYmISCaUYEREJBNKMIPjqkoH0AfFtWUU15bbXmNTXFsmk7jUByMiIplQDUZERDKhBCMiIplQgtkGZnaCmT1nZvPN7NwKx/KymT1lZk+Y2exYNsbM7jWz5+Pz6CGK5admtszMni4r6zMWMzsvbsPnzOz4IY7rAjN7NW63J8zsPRWIa7KZ/cHM5pnZXDP7fCyv6DbrJ66KbjMzqzWzx8zsbzGub8TySm+vvuKq+GcsvlfOzB43s9vj6+y3l7vrsRUPwhWaXwBeB1QDfwMOqGA8LwNje5R9Gzg3Dp8LXDREsRwNHA48vblYgAPitqsB9o7bNDeEcV0AfLGXaYcyrvHA4XF4OPD3+P4V3Wb9xFXRbUa4meCwOFwFPAq8ZTvYXn3FVfHPWHy//wB+CdweX2e+vVSD2XpHAvPd/UV3bwduBKZVOKaepgHXxuFrgROH4k3d/SFg5QBjmQbc6O5t7v4SMJ+wbYcqrr4MZVxL3P2vcXgdMI9we++KbrN+4urLUMXl7t4SX1bFh1P57dVXXH0Zss+YmU0C3gv8pMf7Z7q9lGC23kRgYdnrRfT/5cuaA783szlmNiOW7e7uSyDsLIBxFYuu71i2h+34b2b2ZGxCKzUTVCQuM5sCHEb49bvdbLMecUGFt1ls7nkCWAbc6+7bxfbqIy6o/GfsUuBLQFpWlvn2UoLZetZLWSWP+X6bux8OvBs408yOrmAsW6LS2/FK4PXAocAS4HuxfMjjMrNhwK3AOe6+tr9JeynLLLZe4qr4NnP3orsfSrgV+pFmdlA/k1c6ropuLzN7H7DM3ecMdJZeyrYqLiWYrbcImFz2ehKwuEKx4O6L4/My4DeEKu1SMxsPEJ+XVSq+fmKp6HZ096Vxp5ACP6arKWBI4zKzKsJO/Bfu/utYXPFt1ltc28s2i7GsBh4ETmA72F69xbUdbK+3AR8ws5cJTfnvNLPrGYLtpQSz9WYBU81sbzOrBqYDMysRiJk1mNnw0jDwT8DTMZ7T42SnA7dVIr6or1hmAtPNrMbM9gamAo8NVVClL1j0QcJ2G9K4zMyAq4F57n5x2aiKbrO+4qr0NjOzRjMbFYfrgOOAZ6n89uo1rkpvL3c/z90nufsUwn7qAXc/jaHYXlkdsbArPID3EI6seQH4SgXjeB3hqI+/AXNLsQC7AfcDz8fnMUMUzw2EpoAOwq+hM/qLBfhK3IbPAe8e4rh+DjwFPBm/WOMrENc/EJogngSeiI/3VHqb9RNXRbcZcDDweHz/p4Gvbe7zXuG4Kv4ZK3u/Y+g6iizz7aVLxYiISCbURCYiIplQghERkUwowYiISCaUYEREJBNKMCIikgklGJFBYGbFsqvlPmGbubq2mX3GzD4xCO/7spmN3Yr5jo9X+R1tZnduaxwivclXOgCRncQGD5cIGRB3/1GWwQzA24E/EK4w/ecKxyI7KSUYkQzFy3PcBLwjFp3q7vPN7AKgxd2/a2ZnA58BCsAz7j7dzMYAPyWcRNsKzHD3J81sN8IJo42Es6ut7L1OA84m3D7iUeBz7l7sEc8pwHlxudOA3YG1ZvZmd/9AFttAdl1qIhMZHHU9mshOKRu31t2PBK4gXNW2p3OBw9z9YEKiAfgG8HgsOx+4LpZ/Hfhfdz+McFb4ngBmtj9wCuGip4cCReBjPd/I3W+i6544bySccX6YkotkQTUYkcHRXxPZDWXPl/Qy/kngF2b2W+C3sewfgA8BuPsDZrabmY0kNGmdFMvvMLNVcfpjgSOAWeESYtTR98VNpxIuAwJQ7+FeLyKDTglGJHvex3DJewmJ4wPA/zWzA+n/kum9LcOAa939vP4CsXA77bFA3syeAcbH+5ec5e5/6n81RLaMmshEsndK2fPD5SPMLAEmu/sfCDeEGgUMAx4iNnGZ2THAcg/3YikvfzdQunnV/cDJZjYujhtjZnv1DMTdm4A7CP0v3yZcGPVQJRfJgmowIoOjLtYESu5299KhyjVm9ijhB91He8yXA66PzV8GXOLuq+NBANeY2ZOETv7SZdW/AdxgZn8F/ggsAHD3Z8zsq4S7miaEK0afCbzSS6yHEw4G+BxwcS/jRQaFrqYskqF4FFmTuy+vdCwiQ01NZCIikgnVYEREJBOqwYiISCaUYEREJBNKMCIikgklGBERyYQSjIiIZOL/A76VUhmmqv+4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the policy performance\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.scatter(x, y, marker='x', c=y)\n",
    "fit = np.polyfit(x, y, deg=4)\n",
    "p = np.poly1d(fit) \n",
    "plt.plot(x,p(x),\"r--\") \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title(' performance ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2320])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=0\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=1\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([980])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=1\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wheel rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=2\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([675])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=2\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=3\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1305])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=3\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=4\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([340])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=4\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=5\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3705])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=5\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=6\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([690])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=6\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "\n",
    "i=7\n",
    "\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1) \n",
    "    step = 0\n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state , i)\n",
    "        # Take action\n",
    "        obs , reward = env(action , list(current_state),step,i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        step += 5\n",
    "        if step == 5005:\n",
    "            step = 0\n",
    "            \n",
    "        current_statei = (current_state[i],current_state[i+9])\n",
    "        new_statei = (new_state[i],new_state[i+9])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "            \n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2005])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i=7\n",
    "step = 0\n",
    "for j in range(50000):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+9])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action , list(current_state),step,i)\n",
    "    step += 5\n",
    "    if step ==5005:\n",
    "        step = 0\n",
    "    current_state = obs\n",
    "    #print (current_state[i] ,action,reward,obs[i] )\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
