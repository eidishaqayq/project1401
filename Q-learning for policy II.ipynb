{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning for policyII\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    st= [0]*16\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list):\n",
    " \n",
    "    # Creating an empty dictionary\n",
    "    freq = {}\n",
    "    for item in my_list:\n",
    "        if (item in freq):\n",
    "            freq[item] += 1\n",
    "        else:\n",
    "            freq[item] = 1\n",
    " \n",
    "    for key, value in freq.items():\n",
    "        print (\"% d : % d\"%(key, value))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env(action,st,i): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[i],weibull_shape[i])\n",
    "    \n",
    "    if action == 0 :\n",
    "        if f <= st[i]: # fail\n",
    "            st[i+8]=1\n",
    "            reward= -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "        else:\n",
    "            #st[i+8]=0\n",
    "            st[i] +=5\n",
    "            reward = 5\n",
    "            \n",
    "    if action ==1 :\n",
    "        if st[i+8] ==0 : \n",
    "            reward = -(time_interval / tp[i])*tp[i]\n",
    "        else :\n",
    "            reward = -(time_interval / tp[i])*time_interval * math.ceil(tf[i]/time_interval)\n",
    "            \n",
    "        st[i]=0\n",
    "        st[i+8]=0\n",
    "            \n",
    "           \n",
    "    return (tuple(st) , reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "min_lr=0.1\n",
    "discount=0.5\n",
    "decay=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon, state,i):\n",
    "    if (state[i+8]==1):\n",
    "        return 1\n",
    "    else:\n",
    "        if (np.random.random() < epsilon):\n",
    "            return random.choice([0,1]) \n",
    "        else:\n",
    "            st = (state[i],state[i+8])\n",
    "            return np.argmax(Q_table[st])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(t):\n",
    "    \n",
    "    \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "    # Learning rate also declines as we add more episodes\n",
    "    return max(min_lr, min(1., 1. - math.log10((t + 1) / decay)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replacement time for tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 0\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate =get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env(action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the policy performance\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.scatter(x, y, marker='x', c=y)\n",
    "fit = np.polyfit(x, y, deg=4)\n",
    "p = np.poly1d(fit) \n",
    "plt.plot(x,p(x),\"r--\") \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title(' performance ')\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2320])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 0\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env(action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 1\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate =get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env(action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([975])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 1\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for wheel rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 2\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([665])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 2\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 3\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1315])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 3\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 4\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([340])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 4\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 5\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3790])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 5\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for steering wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 6\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(400):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([700])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 6\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimal replacement time for shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "\n",
    "i = 7\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state,i)\n",
    "        # Take action\n",
    "        obs,reward = env (action,list(current_state),i)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        \n",
    "        current_statei = (current_state[i],current_state[i+8])\n",
    "        new_statei = (new_state[i],new_state[i+8])\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_statei][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_statei]) \n",
    "                                         - Q_table[current_statei][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2005])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset()\n",
    "i = 7\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[(current_state[i],current_state[i+8])])\n",
    "    if action==1:\n",
    "        optimal.append(current_state[i])\n",
    "    # Take action\n",
    "    obs,reward = env (action,list(current_state),i)\n",
    "    current_state = obs\n",
    "np.unique(optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
